{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYV4XcjmSCrZkoU61x028F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saimon0007/Data-Processing/blob/main/Downstram_Pipeline_of_Proposed_Method(Retail_Customer_churn).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFOyzSQUUR2y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#from sklearn.metrics import (accuracy_score, confusion_matrix, roc_auc_score, f1_score)\n",
        "from sklearn.metrics import (confusion_matrix, accuracy_score, f1_score, roc_auc_score, recall_score, precision_score)\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import neighbors\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier,\n",
        "    GradientBoostingClassifier,\n",
        "    ExtraTreesClassifier,\n",
        "    VotingClassifier,\n",
        "    StackingClassifier,\n",
        "    AdaBoostClassifier,\n",
        "    ExtraTreesClassifier,\n",
        "    BaggingClassifier\n",
        ")\n",
        "\n",
        "from sklearn.linear_model import (\n",
        "    LogisticRegression,\n",
        "    RidgeClassifier,\n",
        "    Perceptron,\n",
        "    SGDClassifier,\n",
        "    PassiveAggressiveClassifier\n",
        ")\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df = pd.read_csv('https://raw.githubusercontent.com/muajnstu/Large_Scale_Implementation_of_DSK_Chain/refs/heads/main/filtered%20data/Saimon%20and%20Roni/Clustered%20retail_customer_churn.csv')\n",
        "\n",
        "X = df.drop(columns=['Target_Churn'])\n",
        "y = df['Target_Churn']\n",
        "\n",
        "X = df.drop(columns=['Target_Churn'])\n",
        "y = df['Target_Churn']"
      ],
      "metadata": {
        "id": "xFYiR-l9mpGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_cat = pd.Categorical(y)\n",
        "y_codes = y_cat.codes"
      ],
      "metadata": {
        "id": "KEuO-kwOvFNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "print(\"Class distribution after SMOTE:\", pd.Series(y_resampled).value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDm1NUJfvL9E",
        "outputId": "70cb8d1d-f0f3-4541-d70d-f38397a98a8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution after SMOTE: Target_Churn\n",
            "0    180\n",
            "2    180\n",
            "1    180\n",
            "3    180\n",
            "4    180\n",
            "5    180\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_resampled, y_resampled, test_size=0.3, random_state=46, stratify=y_resampled\n",
        ")"
      ],
      "metadata": {
        "id": "6tYpehGGvOC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementation**"
      ],
      "metadata": {
        "id": "HgvoWTa_Eb3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_metrics(y_true, y_pred, y_prob=None):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    num_classes = cm.shape[0]\n",
        "\n",
        "    if num_classes == 2:\n",
        "        TN, FP, FN, TP = cm.ravel()\n",
        "        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
        "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "        gmean = np.sqrt(specificity * sensitivity)\n",
        "        type1 = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
        "        type2 = FN / (TP + FN) if (TP + FN) > 0 else 0\n",
        "        fmeasure = f1_score(y_true, y_pred, pos_label=1)\n",
        "        auc = 0\n",
        "        if y_prob is not None and hasattr(y_prob, \"shape\") and y_prob.shape[1] > 1:\n",
        "            try:\n",
        "                auc = roc_auc_score(y_true, y_prob[:, 1])\n",
        "            except Exception:\n",
        "                auc = 0\n",
        "    else:\n",
        "        TP = np.diag(cm)\n",
        "        FP = np.sum(cm, axis=0) - TP\n",
        "        FN = np.sum(cm, axis=1) - TP\n",
        "        TN = np.sum(cm) - (FP + FN + TP)\n",
        "\n",
        "        specificity = np.mean([\n",
        "            TN[i] / (TN[i] + FP[i]) if (TN[i] + FP[i]) > 0 else 0 for i in range(num_classes)\n",
        "        ])\n",
        "        sensitivity = np.mean([\n",
        "            TP[i] / (TP[i] + FN[i]) if (TP[i] + FN[i]) > 0 else 0 for i in range(num_classes)\n",
        "        ])\n",
        "        gmean = np.sqrt(specificity * sensitivity)\n",
        "        type1 = np.mean([\n",
        "            FP[i] / (FP[i] + TN[i]) if (FP[i] + TN[i]) > 0 else 0 for i in range(num_classes)\n",
        "        ])\n",
        "        type2 = np.mean([\n",
        "            FN[i] / (TP[i] + FN[i]) if (TP[i] + FN[i]) > 0 else 0 for i in range(num_classes)\n",
        "        ])\n",
        "        fmeasure = f1_score(y_true, y_pred, average='macro')\n",
        "        auc = 0\n",
        "        if y_prob is not None and hasattr(y_prob, \"shape\") and y_prob.shape[1] > 1:\n",
        "            try:\n",
        "                auc = roc_auc_score(y_true, y_prob, multi_class='ovr', average='macro')\n",
        "            except Exception:\n",
        "                auc = 0\n",
        "\n",
        "    print(f\"Accuracy      : {accuracy:.4f}\")\n",
        "    print(f\"Sensitivity   : {sensitivity:.4f}\")\n",
        "    print(f\"Specificity   : {specificity:.4f}\")\n",
        "    print(f\"G-Mean        : {gmean:.4f}\")\n",
        "    print(f\"Type I Error  : {type1:.4f}\")\n",
        "    print(f\"Type II Error : {type2:.4f}\")\n",
        "    print(f\"F1 Score      : {fmeasure:.4f}\")\n",
        "    print(f\"AUROC         : {auc:.4f}\")"
      ],
      "metadata": {
        "id": "YEEIm9fEvRok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model(name, model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    try:\n",
        "        y_prob = model.predict_proba(X_test)\n",
        "    except Exception:\n",
        "        y_prob = None\n",
        "    print(f\"\\nModel: {name}\")\n",
        "    print_metrics(y_test, y_pred, y_prob)"
      ],
      "metadata": {
        "id": "HWyMr_n0vafR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ml_models = {\n",
        "    # Original models\n",
        "    \"RandomForest\": RandomForestClassifier(random_state=42),\n",
        "    \"ExtraTrees\": ExtraTreesClassifier(random_state=42),\n",
        "    \"Bagging\": BaggingClassifier(random_state=42),\n",
        "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
        "    \"LogisticRegression\": LogisticRegression(max_iter=5000, random_state=42, solver='saga'),\n",
        "    \"RidgeClassifier\": RidgeClassifier(random_state=42),\n",
        "    \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"NaiveBayes\": GaussianNB(),\n",
        "    \"Perceptron\": Perceptron(random_state=42),\n",
        "    \"SGDClassifier\": SGDClassifier(random_state=42),\n",
        "    #\"KNN\": KNeighborsClassifier(n_neighbors=3),\n",
        "    \"PassiveAggressive\": PassiveAggressiveClassifier(random_state=42),\n",
        "    #\"RBFSVM\": SVC(kernel='rbf', probability=True, random_state=42),\n",
        "    \"LDA\": LinearDiscriminantAnalysis(),\n",
        "    \"QDA\": QuadraticDiscriminantAnalysis(),\n",
        "    \"LightGBM\": LGBMClassifier(verbosity=-1, random_state=42),\n",
        "\n",
        "\n",
        "    \"VotingSoft\": VotingClassifier(\n",
        "        estimators=[\n",
        "            ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "            ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
        "            ('et', ExtraTreesClassifier(n_estimators=100, random_state=42)),\n",
        "            ('lgbm', LGBMClassifier(verbosity=-1, n_estimators=100, random_state=42))\n",
        "        ],\n",
        "        voting='soft',\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "\n",
        "\n",
        "    \"VotingHard\": VotingClassifier(\n",
        "        estimators=[\n",
        "            ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "            ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
        "            ('et', ExtraTreesClassifier(n_estimators=100, random_state=42)),\n",
        "            ('lgbm', LGBMClassifier(verbosity=-1, n_estimators=100, random_state=42))\n",
        "        ],\n",
        "        voting='hard',\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "\n",
        "\n",
        "    \"VotingWeighted\": VotingClassifier(\n",
        "        estimators=[\n",
        "            ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "            ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
        "            ('et', ExtraTreesClassifier(n_estimators=100, random_state=42)),\n",
        "            ('lgbm', LGBMClassifier(verbosity=-1, n_estimators=100, random_state=42)),\n",
        "            ('xgb', XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss', use_label_encoder=False))\n",
        "        ],\n",
        "        voting='soft',\n",
        "        weights=[2, 3, 2, 3, 3],\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "\n",
        "\n",
        "    \"Stacking\": StackingClassifier(\n",
        "        estimators=[\n",
        "            ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "            ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
        "            ('et', ExtraTreesClassifier(n_estimators=100, random_state=42)),\n",
        "            ('lgbm', LGBMClassifier(verbosity=-1, n_estimators=100, random_state=42)),\n",
        "            ('xgb', XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss', use_label_encoder=False))\n",
        "        ],\n",
        "        final_estimator=LogisticRegression(max_iter=5000, random_state=42, solver='saga'),\n",
        "        cv=5,\n",
        "        stack_method='predict_proba',\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "}"
      ],
      "metadata": {
        "id": "rXjRH9YtveQ5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ml_models[\"LDA\"] = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
        "ml_models[\"QDA\"] = QuadraticDiscriminantAnalysis(reg_param=0.01)\n",
        "\n",
        "for name, model in ml_models.items():\n",
        "    run_model(name, model, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vu3SEpRvizQ",
        "outputId": "8feb0aed-cce1-4453-ecf8-ab2543296c33"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: RandomForest\n",
            "Accuracy      : 0.5463\n",
            "Sensitivity   : 0.5463\n",
            "Specificity   : 0.9093\n",
            "G-Mean        : 0.7048\n",
            "Type I Error  : 0.0907\n",
            "Type II Error : 0.4537\n",
            "F1 Score      : 0.5450\n",
            "AUROC         : 0.9106\n",
            "\n",
            "Model: ExtraTrees\n",
            "Accuracy      : 0.5185\n",
            "Sensitivity   : 0.5185\n",
            "Specificity   : 0.9037\n",
            "G-Mean        : 0.6845\n",
            "Type I Error  : 0.0963\n",
            "Type II Error : 0.4815\n",
            "F1 Score      : 0.5183\n",
            "AUROC         : 0.8908\n",
            "\n",
            "Model: Bagging\n",
            "Accuracy      : 0.5278\n",
            "Sensitivity   : 0.5278\n",
            "Specificity   : 0.9056\n",
            "G-Mean        : 0.6913\n",
            "Type I Error  : 0.0944\n",
            "Type II Error : 0.4722\n",
            "F1 Score      : 0.5213\n",
            "AUROC         : 0.8985\n",
            "\n",
            "Model: GradientBoosting\n",
            "Accuracy      : 0.5247\n",
            "Sensitivity   : 0.5247\n",
            "Specificity   : 0.9049\n",
            "G-Mean        : 0.6891\n",
            "Type I Error  : 0.0951\n",
            "Type II Error : 0.4753\n",
            "F1 Score      : 0.5210\n",
            "AUROC         : 0.9110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: LogisticRegression\n",
            "Accuracy      : 0.4321\n",
            "Sensitivity   : 0.4321\n",
            "Specificity   : 0.8864\n",
            "G-Mean        : 0.6189\n",
            "Type I Error  : 0.1136\n",
            "Type II Error : 0.5679\n",
            "F1 Score      : 0.4286\n",
            "AUROC         : 0.8607\n",
            "\n",
            "Model: RidgeClassifier\n",
            "Accuracy      : 0.4012\n",
            "Sensitivity   : 0.4012\n",
            "Specificity   : 0.8802\n",
            "G-Mean        : 0.5943\n",
            "Type I Error  : 0.1198\n",
            "Type II Error : 0.5988\n",
            "F1 Score      : 0.3746\n",
            "AUROC         : 0.0000\n",
            "\n",
            "Model: DecisionTree\n",
            "Accuracy      : 0.5556\n",
            "Sensitivity   : 0.5556\n",
            "Specificity   : 0.9111\n",
            "G-Mean        : 0.7115\n",
            "Type I Error  : 0.0889\n",
            "Type II Error : 0.4444\n",
            "F1 Score      : 0.5523\n",
            "AUROC         : 0.7333\n",
            "\n",
            "Model: NaiveBayes\n",
            "Accuracy      : 0.5216\n",
            "Sensitivity   : 0.5216\n",
            "Specificity   : 0.9043\n",
            "G-Mean        : 0.6868\n",
            "Type I Error  : 0.0957\n",
            "Type II Error : 0.4784\n",
            "F1 Score      : 0.5199\n",
            "AUROC         : 0.9109\n",
            "\n",
            "Model: Perceptron\n",
            "Accuracy      : 0.3117\n",
            "Sensitivity   : 0.3117\n",
            "Specificity   : 0.8623\n",
            "G-Mean        : 0.5185\n",
            "Type I Error  : 0.1377\n",
            "Type II Error : 0.6883\n",
            "F1 Score      : 0.2288\n",
            "AUROC         : 0.0000\n",
            "\n",
            "Model: SGDClassifier\n",
            "Accuracy      : 0.3395\n",
            "Sensitivity   : 0.3395\n",
            "Specificity   : 0.8679\n",
            "G-Mean        : 0.5428\n",
            "Type I Error  : 0.1321\n",
            "Type II Error : 0.6605\n",
            "F1 Score      : 0.2261\n",
            "AUROC         : 0.0000\n",
            "\n",
            "Model: PassiveAggressive\n",
            "Accuracy      : 0.2654\n",
            "Sensitivity   : 0.2654\n",
            "Specificity   : 0.8531\n",
            "G-Mean        : 0.4759\n",
            "Type I Error  : 0.1469\n",
            "Type II Error : 0.7346\n",
            "F1 Score      : 0.1369\n",
            "AUROC         : 0.0000\n",
            "\n",
            "Model: LDA\n",
            "Accuracy      : 0.5309\n",
            "Sensitivity   : 0.5309\n",
            "Specificity   : 0.9062\n",
            "G-Mean        : 0.6936\n",
            "Type I Error  : 0.0938\n",
            "Type II Error : 0.4691\n",
            "F1 Score      : 0.5294\n",
            "AUROC         : 0.9085\n",
            "\n",
            "Model: QDA\n",
            "Accuracy      : 0.5278\n",
            "Sensitivity   : 0.5278\n",
            "Specificity   : 0.9056\n",
            "G-Mean        : 0.6913\n",
            "Type I Error  : 0.0944\n",
            "Type II Error : 0.4722\n",
            "F1 Score      : 0.5270\n",
            "AUROC         : 0.8990\n",
            "\n",
            "Model: LightGBM\n",
            "Accuracy      : 0.5525\n",
            "Sensitivity   : 0.5525\n",
            "Specificity   : 0.9105\n",
            "G-Mean        : 0.7092\n",
            "Type I Error  : 0.0895\n",
            "Type II Error : 0.4475\n",
            "F1 Score      : 0.5525\n",
            "AUROC         : 0.9129\n",
            "\n",
            "Model: VotingSoft\n",
            "Accuracy      : 0.5370\n",
            "Sensitivity   : 0.5370\n",
            "Specificity   : 0.9074\n",
            "G-Mean        : 0.6981\n",
            "Type I Error  : 0.0926\n",
            "Type II Error : 0.4630\n",
            "F1 Score      : 0.5370\n",
            "AUROC         : 0.9140\n",
            "\n",
            "Model: VotingHard\n",
            "Accuracy      : 0.5494\n",
            "Sensitivity   : 0.5494\n",
            "Specificity   : 0.9099\n",
            "G-Mean        : 0.7070\n",
            "Type I Error  : 0.0901\n",
            "Type II Error : 0.4506\n",
            "F1 Score      : 0.5477\n",
            "AUROC         : 0.0000\n",
            "\n",
            "Model: VotingWeighted\n",
            "Accuracy      : 0.5494\n",
            "Sensitivity   : 0.5494\n",
            "Specificity   : 0.9099\n",
            "G-Mean        : 0.7070\n",
            "Type I Error  : 0.0901\n",
            "Type II Error : 0.4506\n",
            "F1 Score      : 0.5493\n",
            "AUROC         : 0.9156\n",
            "\n",
            "Model: Stacking\n",
            "Accuracy      : 0.5679\n",
            "Sensitivity   : 0.5679\n",
            "Specificity   : 0.9136\n",
            "G-Mean        : 0.7203\n",
            "Type I Error  : 0.0864\n",
            "Type II Error : 0.4321\n",
            "F1 Score      : 0.5629\n",
            "AUROC         : 0.9159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import contextlib\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Capture printed output\n",
        "buffer = io.StringIO()\n",
        "\n",
        "with contextlib.redirect_stdout(buffer):\n",
        "    for name, model in ml_models.items():\n",
        "        run_model(name, model, X_train, X_test, y_train, y_test)\n",
        "\n",
        "output_text = buffer.getvalue()   # <-- now it DEFINITELY exists\n",
        "\n",
        "# 2) Parse output into rows\n",
        "rows = []\n",
        "current_model = None\n",
        "\n",
        "for line in output_text.splitlines():\n",
        "    line = line.strip()\n",
        "\n",
        "    if line.startswith(\"Model:\"):\n",
        "        current_model = {\"Model\": line.split(\":\", 1)[1].strip()}\n",
        "        rows.append(current_model)\n",
        "\n",
        "    elif \":\" in line and current_model is not None:\n",
        "        key, value = line.split(\":\", 1)\n",
        "        try:\n",
        "            current_model[key.strip()] = float(value.strip())\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "# 3) Export to CSV\n",
        "df = pd.DataFrame(rows)\n",
        "df.to_csv(\"model_outputs.csv\", index=False)\n",
        "\n",
        "print(\"Saved model_outputs.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vM69rQm2m5j",
        "outputId": "419becb1-0144-45a5-a18b-8049eb3b4c52"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model_outputs.csv\n"
          ]
        }
      ]
    }
  ]
}